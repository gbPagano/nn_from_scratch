{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rich.progress import track\n",
    "\n",
    "class Tanh:\n",
    "    def activate(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return 1 - self.activate(x) ** 2\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, len_inputs, neurons, function, last=False):\n",
    "        shape = neurons, len_inputs + 1\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, size=shape)\n",
    "        self.f = function\n",
    "        self.last = last\n",
    "        self.idx = None\n",
    "        self.neurons = neurons\n",
    "        self.len_inputs = len_inputs\n",
    "    \n",
    "    def forward(self, layer_input):\n",
    "        self.input = layer_input\n",
    "        self.net = self.input.dot(self.weights.T)\n",
    "        self.output = self.f.activate(self.net)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, alpha, previous_delta=None, previous_weigth=None, error=None):\n",
    "        if self.last:\n",
    "            self.delta = error * self.f.derivative(self.net)\n",
    "        else:\n",
    "            self.delta = (np.delete(previous_delta.dot(previous_weigth).T, 0) * self.f.derivative(self.net))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        self.weights += np.array([self.delta]).T * np.array([self.input]) * alpha\n",
    "        \n",
    "        return self.delta, self.weights\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"({self.idx}ยบ Layer, Neurons: {self.neurons}, Last: {self.last})\"\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, *layers: Layer):\n",
    "        self.layers = list(layers)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            layer.idx = idx + 1\n",
    "        self.layers[-1].last = True\n",
    "        self.len_inputs = self.layers[0].len_inputs\n",
    "        self.all_mse = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"NeuralNetwork (Num_Layers: {len(self.layers)}, Len_Inputs: {self.len_inputs}, Layers: {self.layers})\"\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        resp = []\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            resp.append((idx+1, layer.weights))\n",
    "        return resp\n",
    "        \n",
    "    def _forward(self, x_input):\n",
    "        #input_layer = x_input\n",
    "        input_layer = np.append(1, x_input)\n",
    "        for layer in self.layers:\n",
    "            out_layer = layer.forward(input_layer)\n",
    "            input_layer = np.append(1, out_layer)\n",
    "            \n",
    "        return out_layer\n",
    "    \n",
    "    def _backward(self, alpha, error):\n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.last:\n",
    "                previous_delta, previous_weigth = layer.backward(alpha, error=error)\n",
    "            else:\n",
    "                previous_delta, previous_weigth = layer.backward(alpha, previous_delta, previous_weigth)\n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs=2000, alpha=0.05, batch_size=1):\n",
    "\n",
    "        for epoch in track(range(epochs), description=\"Processing...\"):\n",
    "            outputs = []\n",
    "            batch_errors = []\n",
    "            data = list(zip(x_train,y_train))\n",
    "            np.random.shuffle(data)\n",
    "            x_train,y_train = zip(*data)\n",
    "            x_train,y_train = np.array(x_train), np.array(y_train)\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                out = self._forward(x)\n",
    "                error = (y - out)\n",
    "\n",
    "                batch_errors.append(error)\n",
    "                if len(batch_errors) == batch_size:\n",
    "                \n",
    "                    batch_error = sum(batch_errors) / batch_size\n",
    "                    batch_errors = []\n",
    "                    self._backward(alpha, batch_error)\n",
    "\n",
    "                outputs.append(out)\n",
    "                \n",
    "            errors = np.array([sum(error) for error in (y_train - outputs) ** 2])\n",
    "            self.mean_squared_error = sum(errors) / len(errors)\n",
    "            self.all_mse.append(self.mean_squared_error)\n",
    "            \n",
    "            if not epoch % 100:\n",
    "                print(f\"MSE: {self.mean_squared_error}\")\n",
    "\n",
    "                \n",
    "                \n",
    "    def predict(self, x):\n",
    "        out = self._forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57636603])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_1 = np.array([\n",
    "    [0.2, 0.4, 0.5],\n",
    "    [0.3, 0.6, 0.7],\n",
    "    [0.4, 0.8, 0.3],\n",
    "])\n",
    "weights_2 = np.array([[-0.7, 0.6, 0.2, 0.7],[-0.3, 0.7, 0.2, 0.8]])\n",
    "weights_3 = np.array([[0.1, 0.8, 0.5]])\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    Layer(2, 3, Tanh()),\n",
    "    Layer(3, 2, Tanh()),\n",
    "    Layer(2, 1, Tanh()),\n",
    ")\n",
    "\n",
    "\n",
    "nn.layers[0].weights = weights_1\n",
    "nn.layers[1].weights = weights_2\n",
    "nn.layers[2].weights = weights_3\n",
    "\n",
    "x_inputs = np.array([[0.3, 0.7]])\n",
    "nn._forward(x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40334683])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn._backward([[-2.0]], 0.05)\n",
    "nn.predict(x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74818302,  0.5718139 ,  0.16392517,  0.66670219],\n",
       "       [-0.31672278,  0.69021751,  0.18747959,  0.7884434 ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta, w = nn.layers[2].backward(0.05, error=[-2.0])\n",
    "delta, w = nn.layers[1].backward(0.05, previous_delta=delta, previous_weigth=w)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.96366032, -0.3344555 ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (np.delete(previous_delta.dot(previous_weigth).T, 0) * self.f.derivative(self.net))\n",
    "\n",
    "np.delete(delta.dot(w).T, 0) * nn.layers[1].f.derivative(nn.layers[1].net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
