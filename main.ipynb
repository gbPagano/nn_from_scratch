{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    ":dep ndarray = \"0.15.6\"\n",
    ":dep ndarray-rand = \"0.14.0\"\n",
    ":dep showata = { version = \"0.3.2\", features=[\"show_ndarray\"]}\n",
    ":dep polars = { version = \"0.29.0\", features=[\"ndarray\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "use ndarray::{Array, Array2, stack, concatenate};\n",
    "use ndarray_rand::RandomExt;\n",
    "use ndarray_rand::rand_distr::Uniform;\n",
    "use ndarray::prelude::*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "use std::f64;\n",
    "\n",
    "#[derive(Debug)]\n",
    "enum ActivationFunction {\n",
    "    Tanh,\n",
    "}\n",
    "\n",
    "impl ActivationFunction {\n",
    "    fn activate(&self, x: f64) -> f64 {\n",
    "        match self {\n",
    "            ActivationFunction::Tanh => self.tanh(x),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    fn derivative(&self, x: f64) -> f64 {\n",
    "        match self {\n",
    "            ActivationFunction::Tanh => self.tanh_derivative(x),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    fn tanh(&self, x: f64) -> f64 {\n",
    "        x.tanh()\n",
    "    }\n",
    "    \n",
    "    fn tanh_derivative(&self, x: f64) -> f64 {\n",
    "        1.0 - self.tanh(x).powi(2)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "#[derive(Debug)]\n",
    "struct Layer {\n",
    "    len_inputs: usize,\n",
    "    neurons: usize,\n",
    "    function: ActivationFunction,\n",
    "    weights: Array2<f64>,\n",
    "    input: Option<Array2<f64>>,\n",
    "    net: Option<Array2<f64>>,\n",
    "    output: Option<Array2<f64>>,\n",
    "    idx: Option<usize>,\n",
    "}\n",
    "\n",
    "impl Layer {\n",
    "    fn new(len_inputs: usize, neurons: usize, function: ActivationFunction) -> Self {\n",
    "        let shape = (neurons, len_inputs + 1);\n",
    "        let weights = Array::random(shape, Uniform::new(-0.5, 0.5));\n",
    "\n",
    "        Layer {\n",
    "            len_inputs: len_inputs,\n",
    "            neurons: neurons,\n",
    "            function: function,\n",
    "            weights: weights,\n",
    "            input: None,\n",
    "            net: None,\n",
    "            output: None,\n",
    "            idx: None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fn forward(&mut self, layer_input: &Array2<f64>) -> Array2<f64> {\n",
    "        self.input = Some(layer_input.clone());\n",
    "        self.net = Some(self.input.as_ref().unwrap().dot(&self.weights.t()));\n",
    "        self.output = Some(self.net.as_ref().unwrap().mapv(|x| self.function.activate(x)));\n",
    "        self.output.clone().unwrap()\n",
    "    }\n",
    "\n",
    "    \n",
    "    fn backward(\n",
    "        &mut self, \n",
    "        alpha: f64,\n",
    "        last: bool,\n",
    "        previous_delta: Option<&Array2<f64>>,\n",
    "        previous_weight: Option<&Array2<f64>>,\n",
    "        error: Option<&Array2<f64>>\n",
    "    ) -> (Array2<f64>, Array2<f64>) {\n",
    "        \n",
    "        let delta = if last {\n",
    "            let error = error.unwrap();\n",
    "            error * self.net.as_ref().unwrap().mapv(|x| self.function.derivative(x))\n",
    "        } else {\n",
    "            let previous_delta = previous_delta.unwrap();\n",
    "            let previous_weight = previous_weight.unwrap();\n",
    "            let delta = previous_delta.dot(previous_weight).slice(s![.., 1..]).to_owned();\n",
    "            delta * self.net.as_ref().unwrap().mapv(|x| self.function.derivative(x))\n",
    "        };\n",
    "        \n",
    "        \n",
    "        self.weights = delta.t().dot(self.input.as_ref().unwrap()) * alpha + &self.weights;\n",
    "        \n",
    "        (delta, self.weights.to_owned())\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    fn set_idx(&mut self, idx: usize) {\n",
    "        self.idx = Some(idx);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "#[derive(Debug)]\n",
    "struct NeuralNetwork {\n",
    "    layers: Vec<Layer>,\n",
    "    all_mse: Vec<f64>,\n",
    "}\n",
    "\n",
    "impl NeuralNetwork {\n",
    "    fn new(mut layers: Vec<Layer>) -> Self {\n",
    "        for (idx, layer) in layers.iter_mut().enumerate() {\n",
    "            layer.set_idx(idx + 1);\n",
    "        }\n",
    "        \n",
    "        NeuralNetwork {\n",
    "            layers,\n",
    "            all_mse: Vec::new(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fn forward(&mut self, x_input: &Array2<f64>) -> Array2<f64> {\n",
    "        let mut input_layer = concatenate![Axis(1), Array::from_shape_vec((1, 1), vec![1.0]).unwrap(), x_input.clone()];\n",
    "        let mut output: Array2<f64> = Array::zeros((0, 0));\n",
    "\n",
    "        for layer in &mut self.layers {\n",
    "            output = layer.forward(&input_layer);\n",
    "            input_layer = concatenate![Axis(1), Array::from_shape_vec((1, 1), vec![1.0]).unwrap(), output];\n",
    "        }\n",
    "\n",
    "        output\n",
    "    }\n",
    "\n",
    "    fn backward(&mut self, alpha: f64, error: &Array2<f64>) {\n",
    "        let mut previous_delta = None;\n",
    "        let mut previous_weight = None;\n",
    "        let mut last = true;\n",
    "        \n",
    "        for layer in self.layers.iter_mut().rev() {\n",
    "            let (delta, weights) = layer.backward(alpha, last, previous_delta.as_ref(), previous_weight.as_ref(), Some(&error));\n",
    "            last = false;\n",
    "            previous_delta = Some(delta);\n",
    "            previous_weight = Some(weights);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    fn predict(&mut self, x: &Array2<f64>) -> Array2<f64> {\n",
    "        self.forward(x)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5763660324621392]], shape=[1, 1], strides=[1, 1], layout=CFcf (0xf), const ndim=2"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let weights_1 = array![[0.2, 0.4, 0.5],[0.3, 0.6, 0.7],[0.4, 0.8, 0.3]];\n",
    "let weights_2 = array![[-0.7, 0.6, 0.2, 0.7],[-0.3, 0.7, 0.2, 0.8]];\n",
    "let weights_3 = array![[0.1, 0.8, 0.5]];\n",
    "\n",
    "\n",
    "let mut nn = NeuralNetwork::new(vec![\n",
    "    Layer::new(2, 3, ActivationFunction::Tanh),\n",
    "    Layer::new(3, 2, ActivationFunction::Tanh),\n",
    "    Layer::new(2, 1, ActivationFunction::Tanh),\n",
    "]);\n",
    "\n",
    "nn.layers[0].weights = weights_1;\n",
    "nn.layers[1].weights = weights_2;\n",
    "nn.layers[2].weights = weights_3;\n",
    "\n",
    "let x_inputs = array![[0.3, 0.7]];\n",
    "let out = nn.forward(&x_inputs);\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.403346832835286]], shape=[1, 1], strides=[1, 1], layout=CFcf (0xf), const ndim=2"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.backward(0.05, &array![[-2.0]]);\n",
    "nn.predict(&x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "use polars::prelude::*;\n",
    "let file_path = \"train.csv\";\n",
    "\n",
    "    // Ler o arquivo CSV para um DataFrame\n",
    "let df: DataFrame = CsvReader::from_path(file_path)\n",
    "    .unwrap()\n",
    "    .infer_schema(None)\n",
    "    .has_header(true)\n",
    "    .finish()\n",
    "    .unwrap();\n",
    "\n",
    "// Exibir o DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    ":dep ndarray-csv = {version = \"0.4.1\"}\n",
    ":dep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "let ndarray = df.to_ndarray::<Float64Type>().unwrap();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0, 0.0, 0.0]], shape=[1, 784], strides=[784, 1], layout=CFcf (0xf), const ndim=2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let mut z = Array::zeros((1, ndarray.shape()[1]-1));\n",
    "for linha in ndarray.slice(s![.., 1..]).to_owned().axis_iter(Axis(0)) {\n",
    "    z.assign(&linha);\n",
    "}\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "The variable `y_train` has type `ArrayBase<ndarray::ViewRepr<&f64>, Dim<[usize; _]>>` which cannot be persisted.\nYou might be able to fix this by creating a `Box<dyn YourType>`. e.g.\nlet v: Box<dyn core::fmt::Debug> = Box::new(foo());\nAlternatively, you can prevent evcxr from attempting to persist\nthe variable by wrapping your code in braces.",
     "output_type": "error",
     "traceback": [
      "The variable `y_train` has type `ArrayBase<ndarray::ViewRepr<&f64>, Dim<[usize; _]>>` which cannot be persisted.\nYou might be able to fix this by creating a `Box<dyn YourType>`. e.g.\nlet v: Box<dyn core::fmt::Debug> = Box::new(foo());\nAlternatively, you can prevent evcxr from attempting to persist\nthe variable by wrapping your code in braces."
     ]
    }
   ],
   "source": [
    "let y_train = ndarray.slice(s![.., ..1]);\n",
    "let x_train = ndarray.slice(s![.., 1..]).to_owned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "ndarray.slice(s![..10, 1..]).to_owned().iter().map(|x| x*2.0).collect::<Vec<f64>>()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "let mut nn = NeuralNetwork::new(vec![\n",
    "    Layer::new(784, 20, ActivationFunction::Tanh),\n",
    "    Layer::new(20, 20, ActivationFunction::Tanh),\n",
    "    Layer::new(20, 10, ActivationFunction::Tanh),\n",
    "]);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.40586170804815636, -0.024131547821892196, -0.44243107583669894, -0.16584735920113047, 0.10727349840600886, 0.07698606477674463, -0.16665875216878479, 0.2790247212350334, 0.4033494545401226, -0.47144016262747224, 0.36555478020993704, -0.4810957879433184, 0.36594020420818363, -0.12174270143021482, -0.41494033382247575, -0.3384689959641929, -0.17831365779862485, -0.05846385876718263, 0.04411660836506481, -0.24785998368543627, 0.04180966107104611],\n",
       " [0.4964477653319561, -0.3470791563261766, -0.2072082184065518, -0.21657467222091142, 0.10653108531627398, 0.2603941293147294, 0.30055876061308373, -0.4863070543958341, 0.05346270687151278, -0.44176736771713654, 0.2170278690101386, -0.27908065697464934, -0.4877812359309186, -0.08193316103218584, 0.4556761955186934, -0.42403803361746295, 0.11094675856660641, -0.17523237885043952, -0.33944500490268603, 0.15043346385477685, -0.463497056476472],\n",
       " [-0.41644619945209893, 0.17696931011206707, 0.2298025343451442, 0.09830706504452102, -0.25550818810890297, -0.29621326177937357, 0.055241683865650204, 0.1936408879734266, 0.099298365895923, 0.32781521557705173, 0.42973969859374694, -0.40976265001399725, -0.02579150562863619, -0.16423462620813023, -0.07848138105368285, 0.3372676506457726, -0.34983846061621016, 0.37552760433496535, 0.23971452084570766, 0.07349705323366407, -0.47073314838795755],\n",
       " [0.008684976985440018, 0.3089096563217171, 0.2781694717325436, 0.47635335705716275, 0.4745857588837121, -0.41043403410235957, -0.4873580056323308, -0.14493398177735073, 0.29355228366273134, 0.36179066456955766, 0.29427283683687877, 0.48659827758113283, -0.26304760829972285, 0.26322529818759177, 0.0005115733197760264, -0.3881012179768144, -0.41835407844486183, 0.4360016666850435, 0.37557672536796294, 0.047537330726997507, 0.06632300391824386],\n",
       " [-0.16929482687951714, 0.2795856298334747, -0.19998312958728692, 0.3534949816223587, 0.2033566702385361, -0.30138365170808523, -0.3390333628079887, 0.41812433315735453, 0.3381062344303474, -0.4783500479894365, 0.2133321639242025, 0.39802496931899767, 0.28691145453073585, 0.2592260867001843, 0.16298594111539844, -0.46340404774979826, -0.10110055336669954, -0.16010993941424134, -0.22943584206911605, -0.4910924050583041, -0.15180685436495756],\n",
       " [-0.4508829081597505, 0.28003048979130507, 0.3709995502899579, 0.10214080044687135, 0.1835488099542959, -0.32217262479745656, 0.40905465226526805, -0.06414730278158043, -0.31123263735036577, 0.3438432492817922, -0.4521198206013426, 0.07430541738331065, -0.2937876443580325, -0.02257392684696624, -0.01895915365407319, -0.4247172809566566, -0.05356729061183518, -0.02715821947713537, 0.33156267269603545, -0.14303956098241732, -0.05080752818855494],\n",
       " [0.31048129408698943, -0.41066656682263547, 0.18335393839791303, -0.28156139756125986, -0.3690689316733369, 0.21050211954333498, -0.4538842158450451, -0.3268999268994519, 0.2333324416416418, 0.4238812186634502, 0.24841785593527477, 0.43738952201736936, 0.029753311377651803, -0.04058259296124045, 0.3763604515609271, -0.1648580429897457, -0.31517295673672385, 0.3835552137304725, 0.40064252580812676, 0.394962015539583, -0.3837843414607651],\n",
       " [-0.3119616651374233, -0.11947713714567554, -0.34617701161153214, 0.3304557919494455, 0.20738139586270066, 0.07128791647500643, -0.34188549094343634, -0.31185866286842057, -0.4298200583870686, -0.38475052403027443, -0.10655422074425513, -0.4104792851186787, 0.4464399465333917, -0.1168788822544049, 0.304987911866051, -0.21233855159417092, 0.015290972156475569, 0.44405733072931586, -0.42876602373214756, -0.1659042874647343, -0.2724996505202364],\n",
       " [-0.2834372804502785, -0.05172128595034997, 0.09281228897133742, 0.05955797294645637, -0.13842118135775405, -0.07347528295647199, 0.43798636472928143, -0.26058420561211104, -0.4664123393897388, -0.19939400892934334, -0.23633034862588587, -0.3806864259499372, 0.3133177082514289, -0.38639679378806857, 0.3834571898261474, -0.00995591648460925, 0.47390075267525344, -0.05751074354923347, 0.15668073772000057, 0.06126853665875065, 0.30058135098526906],\n",
       " [-0.22978850031874654, 0.026369351556151388, -0.14856690154323826, -0.4938253109041617, 0.1759094979167426, -0.09052900027246036, -0.013890219350038535, -0.31396272866679453, -0.4811814013991873, 0.44783810963936865, 0.0741576608594332, -0.3928251335649233, -0.4909856846844318, 0.28829045601411973, 0.17302809237989947, 0.4422136130090486, 0.3183038227876305, -0.10874102060049995, -0.004043116695854554, -0.4002544171897646, -0.3388421397123331]], shape=[10, 21], strides=[21, 1], layout=Cc (0x5), const ndim=2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.8575975543026888, 0.7418844757167533, 0.008276932948479847, 0.30835947317996804, -0.9020352534626893, 0.7557398196992497, 0.7035063310380174, -0.7711807842923268, 0.5960649283081518, 0.9926227474377259]], shape=[1, 10], strides=[1, 1], layout=CFcf (0xf), const ndim=2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward(&z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
